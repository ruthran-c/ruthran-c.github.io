---
title: Quantifying the Comparison and Divergence of 2 Distributions
date: 2024-02-20 233500 -0500
categories: [TIL, Data Science]
tags: [python]
---

The capability to quantify the disparity between a **distribution P** and a 
reference **distribution Q** has many useful applications across 
machine learning and data science:

1. Categorical and numerical data drift in ML models.
2. The **_matching_** of a real data distribution with a reference (scheduling templates).
3. Information loss when replacing real data with a statistical approximation (normal distribution).


## References
[^]: [](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
[^]: [](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8)
[^]: [](https://www.giskard.ai/knowledge/how-to-test-ml-models-2-n-categorical-data-drift)
[^]: [](https://stackoverflow.com/questions/45086712/how-to-kullback-leibler-divergence-of-two-datasets)
[^]: [](https://mathoverflow.net/questions/428245/relationship-between-kl-chi-squared-and-hellinger)
[^]: [](https://math.stackexchange.com/questions/2674547/chi-squared-divergence-and-kullback-leibler-divergence)